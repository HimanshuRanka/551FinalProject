# COMP 551 mini-project 4

This is the codebase of all the code we used to collect and test the data
when trying to reproduce the results of the WantWords paper. 

The file structure is as follows

```bash
    root
    │   ...
    │   compare_synonyms.py       # Run this script to get the mean synonym for 
    |                             # top 10 25 50 1000 respectively
    │   paper_metrics.py          # Calculates all the metrics in the paper other than synonyms
    │   scrape_one_look.py        
    │   scrape_wantwords.py
    │
    ├───data
    │       data_desc_c.json      # Orig set of 200 description word pairs
    │       data_test_500_rand1_seen.json     # orig set of 500 seen definition word pairs
    │       data_test_500_rand1_unseen.json   # orig set of 500 unseen definition word pairs
    │       user_gen_defs.txt     # descriptions for alternative dataset
    │       user_gen_target.txt   # target words for alternatively constructed dataset
    │
    └───results                   # ol - One look results ww - want word results
            data_desc_results_ol.json
            data_desc_results_ww.json
            data_seen_results_ol.json
            data_seen_results_ww.json
            data_unseen_results_ol.json
            data_unseen_results_ww.json
            user_gen_defs_results_ol.json
            user_gen_defs_results_ww.json
```

## Scraping

The process of scraping essentially included feeding in
all the descriptions for each data set and the creating
a list of lists of the words generated by the websites
individually. 

all files can be run using

the scraping files will scrape the data for each of the 
datasets and store it in the right file in the results folder.

```commandline
python file_name.py
```

The test files will all print out the results directly
to stdout. The results can be sorted based on file name

